<!--
  Copyright (C) 2004 by Institute for Systems Biology,
  Seattle, Washington, USA.  All rights reserved.
  
  This document is distributed under the GNU Lesser 
  General Public License, the text of which is available at:
    http://www.gnu.org/copyleft/lesser.html
 -->

<!-- ========== INTRODUCTION ========== -->

<isb:docsection name="Introduction" label="introduction">
<isb:docsubsection name="About &appName;" label="about">
<p>
&appName; is a collection of programs for inferring the set elements affected 
by a perturbation of a biological system, based on a collection 
of evidences.  It contains four programs:  
<a href="#datamanager">Data Manager</a>,
<a href="#quantilenormalizer">Quantile Normalizer</a>,
<a href="#significancecalculator">Significance Calculator</a>,
and 
<a href="#evidenceweightedinferer">Evidence-Weighted Inferer</a>.
</p>
<p>
This document is the user manual for the &appName; program.
This manual applies to the following release version of the program:
<blockquote>
<pre>
release version:   &appVersion;
release date:      &appDate;
</pre>
</blockquote>
The README file for this version of the program can be found
at the following URL:
<blockquote>
<a href="&appHomePage;/docs/ReadMe.html">&appHomePage;/docs/ReadMe.html</a>
</blockquote>
The home page for this program is:
<blockquote>
<a href="&appHomePage;">&appHomePage;</a>
</blockquote>
The version history for this program can be found at the following URL:
<blockquote>
<a href="&appHomePage;/docs/VersionHistory.html">&appHomePage;/docs/VersionHistory.html</a>
</blockquote>
If you are reading this document through a print-out, you can find
the online version of this document (which may be a more recent version)
at the following URL:
<blockquote>
<a href="&appHomePage;/docs/UserManual.html">&appHomePage;/docs/UserManual.html</a>
</blockquote>
A PDF version of this manual is also available on-line at:
<blockquote>
<pre>
<a href="&appHomePage;/docs/UserManual.pdf">&appHomePage;/docs/UserManual.pdf</a>
</pre>
</blockquote>
The above hyperlinks for the User Manual are for the most recent
version of the &appName; system.
</p>
</isb:docsubsection>
<isb:docsubsection name="External Libraries" label="extlib">
<p>
The &appName; system relies upon a number of external open-source
libraries.  These libraries are bundled with the &appName; program and
are installed within the &appName; directory when you install
&appName; on your system.   
</p>

<p>
The following table documents the external library dependencies of
the &appName; system.  The libraries are provided in a compiled
format called a &quot;JAR archive&quot;.  Some of the libraries have
software licenses that require making the source code available, namely,
the GNU Lesser General Public License (LGPL).  For each of those licenses,
a hyperlink is provided to a compressed archive file containing the
source code for the version of the library that is included with &appName;.
These hyperlinks are shown in the &quot;Source&quot; column below.
<blockquote>
<table border="1">
<tr>
<td>Package name</td><td>JAR name</td><td>Home Page / Documentation</td><td>License</td><td>Version</td><td>Source Code</td>
</tr>

<tr>
<td>JavaHelp</td><td><code>jh.jar</code></td>
<td><a href="http://java.sun.com/products/javahelp"><code>http://java.sun.com/products/javahelp</code></a></td>
<td>Sun Binary Code License</td>
<td>1.1.3</td>
<td><a href="http://java.sun.com/products/javahelp">partial</a></td>
</tr>

<tr>
<td>colt</td><td><code>colt.jar</code></td>
<td><a href="http://hoschek.home.cern.ch/hoschek/colt"><code>http://hoscheck.home.cern.ch/hoscheck/colt</code></a></td>
<td>open source (see below)</td>
<td>1.0.3</td>
<td><a href="&publicRepositoryURL;/src/colt1.0.3.zip">full</a></td>
</tr>

</table>
</blockquote>
</p>

<p>
The Colt library is provided under the following license terms:
<blockquote>
<pre>
Copyright (c) 1999 CERN - European Organization for Nuclear Research.
Permission to use, copy, modify, distribute and sell this software and its documentation for any purpose
is hereby granted without fee, provided that the above copyright notice appear in all copies and
that both that copyright notice and this permission notice appear in supporting
documentation.
CERN makes no representations about the suitability of this software for any purpose.
It is provided "as is" without expressed or implied warranty.
</pre>
</blockquote>
</p>


</isb:docsubsection>

<isb:docsubsection name="Acknowledgements" label="acknowledgements">
<p>
The assistance, advice, and contributions of several individuals 
to the &appName; project is gratefully acknowledged.
</p>
<p>
Hamid Bolouri:  principal investigator 
</p>
<p>
Daehee Hwang:  for developing the Pointillist algorithm
</p>
<p>
Larissa Kamenkovich:  for implementing an early version of the 
Pointillist program
</p>
<p>
William Longabaugh:  for providing frequent advice on Java
</p>
<p>
Many other individuals have contributed to the project, as well.
In particular it should be noted that &appName; makes extensive use of 
<a href="#extlib">external libraries</a>. The &appName; system would 
not have been possible without the hard work and contributions of the 
authors of these libraries.
</p>

</isb:docsubsection>
</isb:docsection>

<isb:docsection name="Getting Started" label="gettingstarted">

<p>
This section describes how to get started with using the &appName;
system.
</p>

<!-- ========== SYSTEM REQUIREMENTS ========== -->

<isb:docsubsection name="System Requirements" label="systemrequirements">
<p>
The &appName; system is implemented in the Java programming language.
This means that an installation of the Java Runtime Environment
(JRE) is required in order to be able to use the &appName; system.

A version of the &appName; system installer program 
(&quot;<code>&appInstallerName;.bin</code>&quot; on Unix/Linux, 
or &quot;<code>&appInstallerName;.exe</code>&quot; on Windows)
is available which has the Sun JRE version 1.4.1 pre-bundled with it.
This is the recommended approach for users who are not knowledgeable
in the use of the Java programming language and runtime environment.
</p>
<p>
You may also download the &quot;thin&quot; version of the installer 
that does not  have the JRE pre-bundled.  In order to use 
the &quot;thin&quot; installation of &appName;, you must already have
a JRE installed on your computer.  The JRE must be at least version 
1.4 or newer, because the software uses Java 1.4 language features and 
extensions.  This software will not function correctly with a 1.3.X 
version of the JRE; if you attempt to run it under a 1.3.X version
of the JRE, you will see an <code>UnsupportedClassVersionException</code>.
</p>
<p>
The specific hardware requirements for using the &appName; system
will vary depending on the complexity of the models being
studied, and on the type of JRE and host operating system.
A good rule of thumb is that at least 512 MB of RAM is recommended.
If you are using your own JRE and it is not a Sun JRE, you will need
to ensure that the appropriate command-line parameters are passed
to the JRE to ensure that the built-in heap size limit is set to
at least 512 MB.  If you are using the Sun JRE, or the JRE that
is pre-bundled with the &appName; installer, this issue does not 
apply to you.
</p>
<p>
This software has been tested with the Sun Java Runtime Environment 
version 1.4.1 on the following platforms: Windows XP Professional on 
the Intel Pentium 4;  Fedora Core 1 Linux on the Intel Pentium 4; 
Mac OSX version 10.2.6 on the PowerPC G4.
It should function properly on most Windows and Linux distributions.
For other operating systems, you may download the &quot;Other Java-Enabled
Platforms&quot; version of the installer.  A Mac OSX version of the
installer is under development and will be released soon.
</p>
<p>
The &appName; installer will install an executable for the
&appName; launcher program specifically designed for the operating
system of the computer on which you are running the installer.
This means that if you run the installer on a Windows computer,
the &appName; launcher that is installed will be a Windows executable.
If there is a need to run &appName; on multiple operating systems
(e.g., in a dual-boot or heterogeneous network-file-system environment),
&appName; should be installed in a separate directory for each
operating system.  One exception applies: it is possible to install 
&appName; on one operating system (e.g., Windows) and run it on a 
different operating system (e.g., Unix), if you are writing your
own Java programs and just using the &localJavaLibraryName; API.
</p>

</isb:docsubsection>

<isb:docsubsection name="Tutorial" label="tutorial">

<p>
&appName; is launched by executing the &quot;&appName;&quot; executable
that was installed as a symbolic link by the installation program.  The
default location of this symbolic link depends on your operating system.
If you are installing on a Windows computer, the symbolic link is created
in a new Program Group &quot;&appName;&quot;, which will show up in the
&quot;Start&quot; menu.  If you are installing on a Linux computer, the
symbolic link is created in your home directory, by default.  Note that
the installation program permits you to override the default location for
the symbolic link to be created, so the symbolic link may not be in the 
default location on your computer, if you selected a different location
in the installation process.  By double-clicking on the &quot;&appName;&quot;
symbolic link, the &appName; program should start up.  You should see an
application window appear that looks like the following picture
</p>

</isb:docsubsection>

</isb:docsection>
  
<isb:docsection name="Data Manager" label="datamanager">

<p>
The <b>Data Manager</b> program enables merging of data from multiple
data files that contain observations for different, partially overlapping 
collections of network elements.  In addition, the Data Manager allows
averaging over multiple measurements of the same evidence type for the
same network element (whether across multiple files or within a single
data file).  The Data Manager displays the data loaded from one or more
data files in a data table, with each row corresponding to a single element
and each column corresponding to a single evidence type.  The table
may be sorted by evidence or element name.   Either the entire table,
or specific selected columns, may be saved to a data file.
</p>  
<p>
The data file of observations that is loaded into the Data Manager must
conform to a specific format.  The file must be in ASCII text, arranged
in columns and rows.  The three allowed delimiters are spaces, tabs, and
commas.  With commas and tabs, an empty cell is allowed, and it is denoted by
two adjacent delimiter characters.  With spaces, empty cells are not allowed,
because multiple adjacent spaces are used to align columns.  A non-empty cell
must contain a floating-point number in either valid scientific notation
(e.g., <code>1.23745E+3</code> or <code>1.23745e+3</code>) or in decimal
notation.  The number must parse successfully as a <code>Double</code> type 
as defined by the <code>java.lang.Double</code> class of the Java programming
language.  The exception to the above rule is the first column and the first row,
which contain alphanumeric identifiers.  The first column contains identifiers
of the network elements (e.g., genes or proteins).  The first row contains
identifiers of evidence types.  Evidence type names must be unique within a file, 
i.e., no duplicate evidence type names are allowed.  The cell at the first row
and first column must contain a valid identifier, but it is not used.  Typically
this cell contains the word "element" or the like.  Note that element names
and evidence names must not contain the character that is used as the delimiter
for the data file.  If your data file is comma-delimited, your element names and
evidence names must not contain the comma character.  To denote missing data,
you can either have an empty cell (which means two delimiter characters appear
back-to-back), or the cell can have the string "null" (without the double-quotes).
Note that the "null" string is case-sensitive, so the string "NULL" will not work.
</p>
<p>
Before you load your data file, please make sure you select the proper
delimiter type, using the "delimiter" drop-down list.  If you attempt to
load a data file with the wrong delimiter type specified, the Data Manager
will likely give an error message, or (less likely) it will display incorrect data
in the data table.  Also, please make sure your data file conforms to the
format specified above.  If it does not, the Data Manager will not be able to load
the data file, or it will (less likely) display incorrect data in the data table.
Once you have loaded the data, it will appear in the data table.  You may 
click on the "load data" button at any time, to load additional data files.
You may change the ordering of the data files, which will rearrange the
order of colums in the data table (if you have not specified a sort
order for evidence names), using the "move file up" button.  The 
"allow duplicates" checkbox allows you to specify how you wish for
the Data Manager to handle the case of multiple measurements of the
same evidence type and network element.  If you have checked this box,
the Data Manager will average over the multiple measurements, and use the
single resulting value as the consensus measurement.  If you have not
checked the box, the Data Manager will pop up an error message and will
not load the file, if the file contains multiple measurements for a single
evidence type and network element.  
</p>
<p>
Here is a screen shot of the Data Manager program:
</p>
<p>
  <a href="../images/DataManager-big.png"><img src="../images/DataManager.png" alt="Data Manager screenshot" /></a>
</p>

</isb:docsection>

<isb:docsection name="Quantile Normalizer" label="quantilenormalizer">

<p>
  The <b>Quantile Normalizer</b> is a program that can be used to perform a quantile
  normalization of microarray expression data that is arranged in a matrix format 
  in a single ASCII data file.  The quantile normalization algorithm
  implemented here is based on a prototype written by Daehee Hwang at 
  Institute for Systems Biology, and it is similar to the quantile normalization 
  algorithm proposed by Bolstat <em>et al.</em> in their paper
  <blockquote>
  Bolstad, B.M., Irizarry R. A., Astrand M., and Speed, T.P. (2003), 
  "A Comparison of Normalization Methods for High Density Oligonucleotide Array Data 
  Based on Bias and Variance." <em>Bioinformatics</em> <b>19</b>(2):185-193
  </blockquote>
  Note that only the quantile normalization step of the RMA (Robust Multi-Chi Average)
  procedure is implemented in this class; background adjustment is not implemented here,
  and is assumed to have been applied to the raw observations before this program 
  is applied to the data.  Each column of the data file corresponds to a different microarray
  experiment, and each row of the data file corresponds to a different probe.
  The first row must contain experiment labels, and each column's experiment label
  must be unique.  The first column must contain probe names, although there is no
  uniqueness requirement for probe names.  The cell in the first row and the first
  column should contain an arbitrary string identifier such as "element", that does not contain
  the file delimiter (the actual string used is not important; the cell is just a placeholder).
  Cells in the data file may be empty, just as described in the <a href="#datamanager">Data Manager</a>
  section above.
</p>
<p>
  The following list describes the various components and controls of the Quantile Normalizer
  program window:
  <dl>
  <dt><b>Load observations</b></dt>
  <dd>This button is used to load a file of data into the program.  The
    data is displayed in the input data table in the top half of the program window.  Only
    one file may be loaded into the input data table at a time.</dd>
  <dt><b>Delimiter</b></dt>
  <dd>This drop-down list is used to specify how your input data file (and output data file)
    are delimited.  The three choices are "tab", "space", and "comma".  Comma-delimited files
    are recommended for reasons of unambiguity and compatibility with spreadsheet programs.
    If you choose "space", be advised that there is a restriction on how you can specify
    an empty cell (missing data).  With a "space" delimiter, an empty cell must be denoted
    by the string "null" (without double quotes).  Note that the string is case-sensitive,
    so you cannot use the string "Null" or "NULL".  Once you have loaded your input data file,
    you may change the delimiter that you use to save the output file, if you wish.  
  </dd>
  <dt><b>Normalize using the scale</b></dt>
  <dd>This drop-down list is used to specify how the data is to be scaled.
    The options are "norm_only" and "logarithm".  If you specify "norm_only",
    only a normalization is performed, and the data is not rescaled.  If
    you specify "logarithm", the logarithm of each observation is taken before
    the normalization is performed.  In addition, the "logarithm" option automatically
    selects the "Fix negative values" check-box (see below), to ensure that
    there are no negative observation values when the logarithm is taken.  The
    "logarithm" option is typically used for normalizing Affymetrix microarray data.</dd>    
  <dt><b>Specify the error tolerance</b></dt>
  <dd>This type-in box is used to specify the error tolerance for estimating
    missing data values.  If there are no missing data in the file of observations
    loaded into the program, this type-in box will be left disabled.  When there
    are missing data values, this type-in box will be enabled and pre-populated
    with a small positive floating-point value.  This value controls the threshold
    for the average fractional error in estimating the missing data values.  The
    average fractional error is computed as the average over all missing observations,
    of the fractional deviation in the interpolated missing observation from one
    iteration to the next.  Missing observations are estimated as the median of
    all non-missing observations in the column (evidence type).  The missing
    observations thus estimated are used only in computing quantiles for normalizing
    the non-missing data; the missing observations will be removed when the normalized
    observations are written out by the program.  If you have a really small data set,
    it is best to set this error tolerance to be a bit larger, perhaps 0.2.  For
    large data sets (more than a few hundred elements), the default value should work.
    The error tolerance must be a positive floating-point number, and less than 1.0.</dd>
  <dt><b>Fix negative values</b></dt>
  <dd>This check-box is used to specify whether the data should be additively corrected
    to ensure that no observations are negative, before the rescaling is applied.  Choosing
    the "logarithm" option for the normalization scale will automatically select this check-box.
    If you leave the box checked, the data will be additively corrected.  If you de-select
    the check-box, the data will not be additively corrected.  Note that if you have nonpositive data 
    values and you selected "logarithm" for the scale and left this box un-checked, you would get an error message when
    you attempt to normalize the data.  If all your data is positive, nothing will be changed
    by this option.  The "norm_only" scale option will not give an error message if there
    are nonpositive observations, and the normalization will be successful in that case.
  </dd>
  <dt><b>Max iterations</b></dt>
  <dd>This type-in box is used to specify the maximum number of iterations that are
    used for estimating missing data values.  If there is no missing data, this
    type-in box is left disabled.  If there are empty cells in the data table,
    this type-in box will be enabled, and will be pre-populated with a small value.
    You may change it to any positive integer value.  This value places a hard limit
    on the number of iterations of the normalization algorithm, regardless of the
    error threshold you selected (see "Specify the error tolerance" above).  If you have
    an extremely large data set, you may wish to select a small number of iterations, perhaps
    in the range 3-5.  Alternatively, you could specify a more permissive error tolerance.
    Note that if you specify an extremely small error tolerance and an extremely large maximum
    number of iterations, the normalization algorithm could run for a very long time; if this
    happens, closing the program's window (using the window manager for your computer) and 
    restarting the program is the best option.
    </dd>
  <dt><b>Normalize raw observations</b></dt>
  <dd>This button starts the normalization.  The normalized observations are displayed
    in a table in the lower half of the program window.</dd>
  <dt><b>Iteration count</b></dt>
  <dd>This box displays the number of iterations required before the normalization
    process converged to the specified error tolerance.  If no cells were empty (denoting
    missing data) in the input data table, the number of iterations will always be 1.
    If there are empty cells in the input data table, the number of iterations may be
    greater than 1.</dd>
  <dt><b>Fractional error</b></dt>
  <dd>This box displays the average fractional error, which is only applicable
    when there are empty cells in the input data table.  If there are empty cells
    in the input data table, the average fractional error is computed as the
    average of the fractional change in the estimated values of the "empty cells" from
    one iteration to the next.  The value displayed in this box may or may not be 
    less than the error threshold specified above, depending on the value chosen for
    the "max iterations" field.</dd>
  </dl>
</p>
<p>
Here is a screen shot of the Quantile Normalizer program:
</p>
<p>
  <a href="../images/QuantileNormalizer-big.png"><img src="../images/QuantileNormalizer.png" alt="Quantile Normalizer screenshot" /></a>
</p>
  
</isb:docsection>

<isb:docsection name="Significance Calculator" label="significancecalculator">
<p>
  The <b>Significance Calculator</b> is a program that can analyze the probability distribution of
  a set of observations, and compute the statistical signficance of each observation on the
  basis of the distribution.  Alternatively, the significances can be computed based on the
  distribution of a separate
  set of "negative control" observations.  The "significance" of an observation is here defined as
  the probability
  that the observation would occur by chance, given either the global or the "negative control" 
  distribution for observations.  The input to the program is a matrix of observations, as described
  in the <a href="#datamanager">Data Manager</a> section.  Missing observations are allowed, 
  and are denoted by an empty cell or the string "null" (quotes not included, case sensitive).
</p>
<p>
  The following list describes the various components and controls of the Significance Calculator
  program window:
  <dl>
  <dt><b>Load observations</b></dt>
  <dd>This button loads a file of observations.  The observations are displayed in the input data
    table, in the upper third of the program window..  Only one file may be 
    loaded into the input data table at a time.</dd>
  <dt><b>Delimiter</b></dt>
  <dd>This drop-down list is used to specify how your input data file (and output data file)
    are delimited.  The three choices are "tab", "space", and "comma".  Comma-delimited files
    are recommended for reasons of unambiguity and compatibility with spreadsheet programs.
    If you choose "space", be advised that there is a restriction on how you can specify
    an empty cell (missing data).  With a "space" delimiter, an empty cell must be denoted
    by the string "null" (without double quotes).  Note that the string is case-sensitive,
    so you cannot use the string "Null" or "NULL".  Once you have loaded your input data file,
    you may change the delimiter that you use to save the output file, if you wish.  
  </dd>
  <dt><b>Evidence choices table</b></dt>
  <dd>This table displays the list of evidence types, and is activated when the input data
    file is loaded.  The five rightmost columns of the table are used to specify how
    significances are to be computed for each type of evidence.  Each column type is
    defined below.</dd>
  <dt><b>Single-tailed</b></dt>
  <dd>This checkbox is used to define whether the observations for a given evidence are
    to be converted to a significance using a single-tailed or two-tailed test.  A 
    single-tailed test is appropriate if the observations are never negative,
    or if the observations are always greater than some minimum value (e.g., 1.0).
    A two-tailed test is appropriate if observations can have large negative or
    large positive values, e.g., logarithm of a fold-change for protein or mRNA
    levels.  The program attempts to guess whether the distribution is single-tailed
    or two-tailed, by looking to see if the evidence type has any negative observations.
    You can override the initial guess by clicking on the "single-tailed" checkbox.</dd>
  <dt><b>Number of bins</b></dt>
  <dd>This text field is used to specify the number of bins that are to be used to
    calculate the nonparametric distribution for the observations for the given 
    evidence type.  The program makes an initial guess at an appropriate number of bins,
    but you may change it to any integer value greater than 1.0.</dd>
  <dt><b>Smoothing length</b></dt>
  <dd>This text field is used to specify the smoothing length to be used; this is the
    standard deviation of the Gaussian used as a kernel density for smoothing the distribution
    of observations.  The program makes an initial guess at an appropriate smoothing lenght,
    but you may change it to any value greater than 0.0.</dd>
  <dt><b>Compute significances</b></dt>
  <dd>This checkbox is used to specify whether significances should be computed
    for this evidence type.  By default, the box is checked, meaning the program
    will compute significances for this evidence type and include it as a column
    in the reusults data table.  If you uncheck this box, the program will not
    compute significances for this data type, and will omit the corresponding column
    from the results data table.</dd>
  <dt><b>Negative control observations</b></dt>
  <dd>This field is used to specify a set of "negative control" observations for
    the evidence type.  It is optional, and by default, set to "&lt;none&gt;".  By
    double-clicking on the field, a file browser is opened and you are prompted
    to select a data file.  The data file should contain negative control data 
    arranged in columns, with each column a different evidence type.  The columns
    may have different numbers of observations in them, and there is no requirement
    of uniformity of rows.  You will be prompted to select a column in a separate
    data table that is displayed, for the negative control file you provided.  If
    you wish to delete your choice of negative control data for a given evidence,
    just select the "negative control" cell for that evidence, and press the
    "delete" key.</dd>
  <dt><b>Calculate the significances using the formula</b></dt>
  <dd>This drop-down list is used to specify how the significances are to be
    computed.   The same method is applied to all evidence types in the
    input data table.  The two methods allowed are "PDF" and "CDF".  Here,
    "PDF" means probability distribution function.  The PDF of the smoothed, normalized 
    nonparametric control distribution is used as the significance of an observation.
    (If no control data was specified for the evidence type, the global distribution 
    is used instead.)  In contrast, "CDF" means cumulative distribution function;
    the significance of an observation is computed as the complement of the CDF 
    of the smoothed, normalized nonparametric distribution function.  For a two-tailed
    evidence type, the "complement of the CDF" here means area under the tail to which the
    observation is closest.  The "CDF" method is also known as the Fisher Method.  The
    "PDF" method is also known as the Bayesian Method.  If you are unsure of which
    method to use, it is recommended to use the default, "CDF", because the
    default settings in the <a href="#evidenceweightedinferer">Evidence-Weighted Inferer</a>
    are appropriate to a "CDF" choice in the Significance Calculator.</dd>
  </dl>
</p>
<p>
Here is a screen shot of the Significance Calculator program:
</p>
<p>
  <a href="../images/SignificanceCalculator-big.png"><img src="../images/SignificanceCalculator.png" alt="Significance Calculator screenshot" /></a>
</p>
</isb:docsection>

<isb:docsection name="Evidence-Weighted Inferer" label="evidenceweightedinferer">
  <p>The <b>Evidence-Weighted Inferer</b> is a classification program that attempts to
    divide a set of elements into two sets, affected and unaffected.  It comparse multiple evidences
    to determine which elements of a network are most likely affected by a perturbation
   of a system.  The input to this program is a matrix of significances for observations
   for evidence types (columns) and network elements (rows).  Missing data is allowed; a
   missing significance value is denoted by the value "-1", or an empty cell in the input
   file.  The smaller a significance
   value for an observation, the <b>more</b> likely it is that the associated element is
   affected by the perturbation of the system; in this sense, the significance is analogous
   to a probability that a given observation would occur, given that the associated element
   is <em>not</em> a member of the set of affected elements.  The significances may
   be calculated using the <a href="#significancecalculator">Significance Calculator</a>,
   or they may be generated by any other procedure that can assign a statistical
   likelihood or probability.  
   Each evidence type is assigned a weight, based on consistency with the
   other types of evidence.  The weights are used to compute an <b>effective significance</b>
   for each significance value in the matrix.  
  </p>
<p>
Here is a screen shot of the Evidence-Weighted Inferer program:
</p>
<p>
  <a href="../images/EvidenceWeightedInferer-big.png"><img src="../images/EvidenceWeightedInferer.png" alt="Evidence-Weighted Inferer screenshot" /></a>
</p>  
</isb:docsection>